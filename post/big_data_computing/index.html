<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  
  









  




  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Onur Copur" />

  
  
  
    
  
  <meta name="description" content="&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; MAP REDUCE &mdash;&mdash;&mdash;&mdash;&mdash;&mdash; Distributed File System (DFS) Modern data-mining applications, often called “big-data” analysis, require us to manage immense amounts of data quickly. The key factor of making fast computation is parallelization and this is done via computer clusters." />

  
  <link rel="alternate" hreflang="en-us" href="https://copuronur.github.io/post/big_data_computing/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.e9dfd0d72348b3575652bf8384f1cedc.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu977448f5f7d0285f18cfefcfd5153e92_24079_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu977448f5f7d0285f18cfefcfd5153e92_24079_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://copuronur.github.io/post/big_data_computing/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Me" />
  <meta property="og:url" content="https://copuronur.github.io/post/big_data_computing/" />
  <meta property="og:title" content="Big Data computing | Me" />
  <meta property="og:description" content="&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; MAP REDUCE &mdash;&mdash;&mdash;&mdash;&mdash;&mdash; Distributed File System (DFS) Modern data-mining applications, often called “big-data” analysis, require us to manage immense amounts of data quickly. The key factor of making fast computation is parallelization and this is done via computer clusters." /><meta property="og:image" content="https://copuronur.github.io/post/big_data_computing/featured.jpg" />
    <meta property="twitter:image" content="https://copuronur.github.io/post/big_data_computing/featured.jpg" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2020-12-13T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2020-12-13T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://copuronur.github.io/post/big_data_computing/"
  },
  "headline": "Big Data computing",
  
  "image": [
    "https://copuronur.github.io/post/big_data_computing/featured.jpg"
  ],
  
  "datePublished": "2020-12-13T00:00:00Z",
  "dateModified": "2020-12-13T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Onur Copur"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://copuronur.github.io/media/icon_hu977448f5f7d0285f18cfefcfd5153e92_24079_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; MAP REDUCE \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; Distributed File System (DFS) Modern data-mining applications, often called “big-data” analysis, require us to manage immense amounts of data quickly. The key factor of making fast computation is parallelization and this is done via computer clusters."
}
</script>

  

  

  

  





  <title>Big Data computing | Me</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="9e8a990b132c24aa2abf204a945ef5ba" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.2da3b1fa37e894630bf6de39b1b694b3.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Me</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Me</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
          
          <li class="nav-item d-none d-lg-inline-flex">
            <a class="nav-link" href="https://twitter.com/copuro_onur" data-toggle="tooltip" data-placement="bottom" title="Follow me on Twitter" target="_blank" rel="noopener" aria-label="Follow me on Twitter">
              <i class="fab fa-twitter" aria-hidden="true"></i>
            </a>
          </li>
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  




















  
  


<div class="article-container pt-3">
  <h1>Big Data computing</h1>

  
  <p class="page-subtitle">Notes from my big data computing class.</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/author/onur-copur/">Onur Copur</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Dec 13, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    25 min read
  </span>
  

  
  
  
  
  
  

  
  

</div>

  





</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 800px; max-height: 796px;">
  <div style="position: relative">
    <img src="/post/big_data_computing/featured.jpg" width="800" height="796" alt="" class="featured-image">
    <span class="article-header-caption">Image credit: <a href="https://unsplash.com/photos/CpkOjOcXdUY"><strong>Unsplash</strong></a></span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="-------------------map-reduce-------------------">&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; MAP REDUCE &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;</h2>
<h2 id="distributed-file-system-dfs">Distributed File System (DFS)</h2>
<p>Modern data-mining applications, often called “big-data” analysis, require us to manage immense amounts of data quickly. The key factor of making fast computation is parallelization and this is done via computer clusters. To exploit cluster computing, files must look and behave somewhat differently from the conventional file systems found on single computers. This new file system is called Distributed File System (DFS).</p>
<ul>
<li>DFS is suitable for enormous, possibly a terabyte in size. If you have only small files, there is no point using a DFS for them.</li>
<li>Files are rarely updated. Rather, they are read as data for some calculation, and possibly additional data is appended to files from time to time.</li>
</ul>
<p>One problem in parallel computing is the machine failures. If you have a large computing clusters, you are likely to observe frequent machine failures. To overcome this challenge, Files are dived into <em>chunks</em> and each <em>chunk</em> is replicated in different nodes (<em>chunk servers</em>) of the cluster. Moreover, the nodes holding copies of one chunk should be located on different racks, so we don’t lose all copies due to a rack failure. In the figure bellow, you can see the diagram of a distributed file system. Note that the chunk servers also serve as compute servers.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/dfsfigure.PNG" alt="DFS structure(chunks and chunk servers)" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<p>The two other components of DFS (which are not shown in figure) are <em>Master Node</em> and <em>Client library</em>. Master node stores meta data about where files are stored and Client library talks to master to find chunk servers. The other problem in big data computing is moving data around for different computations is computationally expensive. The way to handle this problem is bring	computation close	to the	data. <strong>MapReduce</strong> is the programming model introduced to handle these problems and <strong>DFS</strong> is the Storage Infrastructure - File system associated with this programming model.</p>
<h2 id="mapreduce-implementation-centralized">MapReduce Implementation (centralized)</h2>
<p>Implementation of MapReduce requires the programmer to write two functions, called <em>Map</em> and <em>Reduce</em>. In brief, a MapReduce
computation executes as follows:</p>
<ol>
<li>
<p>Some number of Map tasks each are given one or more chunks from a DFS. These Map tasks turn the chunk into a sequence of <em>key-value</em> pairs.</p>
</li>
<li>
<p>The key-value pairs from each Map task are collected by a <em>master controller</em> and sorted by key. The keys are divided among all the Reduce tasks, so all key-value pairs with the same key wind up at the same Reduce task.</p>
</li>
<li>
<p>The Reduce tasks work on one key at a time, and combine all the values associated with that key in some way.</p>
</li>
</ol>
<p>In the diagram bellow, you can see a MapReduce implementation for the word count task. Note that this scheme shows how MapReduce works in a centralized system.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/Capture.PNG" alt="Map reduce diagram centralized" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h2 id="mapreduce-implementation-parallel">MapReduce Implementation (Parallel)</h2>
<p>However we are more interested in MapReduce in a computer cluster. In a computer cluster Map and Reduce tasks are running in parallel on multiple nodes. As we mentioned before, same data chunks can appear in different nodes and different Map tasks maybe processing same data chunks. But for the reduce tasks, we want put all key-value pairs with same key to the same reduce task. Here we introduce the <em>Partitioning Function</em> which is just a hash function responsible from this operation. In the figure bellow, you can see the MapReduce reduce diagram in parallel.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/mapreduceparallel.PNG" alt="Map reduce diagram in computer cluster" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h3 id="mapreduce-environment-duties">MapReduce environment duties:</h3>
<ul>
<li>Partitioning the input data.</li>
<li>Scheduling the program&rsquo;s execution across a set of machines.</li>
<li>Performing the <em>group by key</em> step.</li>
<li>Handling machine failures.</li>
<li>Managing required intermachine communication.</li>
</ul>
<h3 id="data-flow">Data Flow</h3>
<ul>
<li>Input and final output are stored on the  DFS.
<ul>
<li>Scheduler tries to schedule map tasks to each chunk server which contains corresponding data chunk.</li>
</ul>
</li>
<li>Intermediate results are stored on local file system of Map and Reduce workers.</li>
</ul>
<h3 id="details-of-mapreduce-execution">Details of MapReduce Execution</h3>
<p>The figure bellow offers an outline of how process, tasks and files interact. Taking advantage of a client library provided by a MapReduce system, the user program forks a <em>Master Controller</em> process and some number of <em>Map workers</em> and <em>Reduce workers</em>. Note that a worker handles either Map tasks or Reduce tasks.</p>
<p>The Master has many responsibilities. One is to create some number of Map tasks and some number of Reduce tasks, these numbers being selected by the user program. These tasks will be assigned to Worker processes by the Master. It is reasonable to create one Map task for every chunk of the input file(s), but we may wish to create fewer Reduce tasks. The reason for limiting the number of Reduce tasks is that it is necessary for each Map task to create an intermediate file for each Reduce task, and if there are too many Reduce tasks the number of intermediate files explodes. The Master keeps track of the status of each Map and Reduce task (idle, executing at a particular Worker, or completed). A Worker process reports to the Master when it finishes a task, and a new task is scheduled by the Master for that Worker process.</p>
<p>Each Map task is assigned one or more chunks of the input file(s) and executes on it the code written by the user. The Map task creates a file for each Reduce task on the local disk of the Worker that executes the Map task. The Master is informed of the location and sizes of each of these files, and the Reduce task for which each is destined. When a Reduce task is assigned by the Master to a Worker process, that task is given all the files that form its input. The Reduce task executes code written by the user and writes its output to a file that is part of the surrounding distributed file system.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/mapreducedetailed.PNG" alt="Map reduce execution" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h3 id="dealing-with-failures">Dealing With Failures</h3>
<ul>
<li>Map worker failure.
<ul>
<li>Map tasks completed or in-progress at worker are rest to idle.</li>
<li>Reduce workers are notified when task is rescheduled on another worker.</li>
</ul>
</li>
<li>Reduce worker failure.
<ul>
<li>Only in-progress tasks are reset to idle.</li>
<li>Idle Reduce tasks restarted on other worker(s).</li>
</ul>
</li>
<li>Master failure.
<ul>
<li>Map reduce task is aborted and client notified.</li>
</ul>
</li>
</ul>
<h3 id="refinements">Refinements</h3>
<h4 id="combiners">Combiners</h4>
<ul>
<li>Often a Map task will produce many key-value pairs with same key. We can save time by pre-aggregating values with same key in the mapper.
<ul>
<li>combine(k, list(v1)) -&gt; v2.</li>
<li>Combiner	is	usually	same as	the	reduce	function.</li>
<li>Works	only	if	reduce function	is	commutative	and	associative.</li>
</ul>
</li>
</ul>
<h4 id="partition-function">Partition Function</h4>
<ul>
<li>Want to control how keys get partitioned.</li>
<li>System uses a default Partition function:
<ul>
<li>hash(key) mod R.</li>
</ul>
</li>
<li>Sometimes useful to override the hash function.</li>
</ul>
<h2 id="algorithms-using-mapreduce">Algorithms Using MapReduce</h2>
<p>The original purpose for which the Google implementation of MapReduce was created was to execute very large matrix-vector multiplications as are needed in the calculation of PageRank.</p>
<h3 id="matrix-vector-multiplication-by-mapreduce">Matrix-Vector Multiplication by MapReduce</h3>
<p>Suppose we have an n x n matrix M, whose element in row i and column j will be denoted m<sub>ij</sub> . Suppose we also have a vector v of length n, whose jth element is v<sub>j</sub> .</p>
<h4 id="the-map-function">The Map function</h4>
<p>Each Map task will operate on a chunk of the matrix M. From each matrix element m<sub>ij</sub> it produces the key-value pair (i,m<sub>ij</sub>v<sub>j</sub>). Thus, all terms of the sum that make up the component x<sub>i</sub> of the matrix-vector product will get the same key, i.</p>
<h4 id="the-reduce-function">The Reduce function</h4>
<p>The Reduce function simply sums all the values associated with a given key i. The result will be a pair (i, x<sub>i</sub>).</p>
<p>Note that parallelization of matrix multiplication is very crucial at neural net computations.</p>
<h3 id="natural-join-by-mapreduce">Natural Join by MapReduce</h3>
<p>Natural join is a frequently used operation in relational database system and parallelization of this operation saves enormous computing time. In the figure bellow yo can see the illustration of Natural Join.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/join.PNG" alt="Natural Join" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<ul>
<li>Use a hash function h from B-values to 1..k</li>
<li>A map process turns:
<ul>
<li>each input tuple R(a,b) into key-value pair (b, (a,R)).</li>
<li>each input tuple S(b,c) into key-value pair (b, (c,S)).</li>
</ul>
</li>
<li>Map process send each key-value pair with key b to Reduce process h(b).</li>
<li>Each reduce process matches all the pairs (b, (a,R)) with all (b, (c,S)) and outputs (a,b,c)</li>
</ul>
<h2 id="cost-measures-for-mapreduce-algorithms">Cost Measures for MapReduce Algorithms</h2>
<h3 id="communication-cost">Communication Cost</h3>
<p>input file size + 2x(sum of the sizes of all files passed from Map process to Reduce process) + the sum of the output sizes of the reduce process.</p>
<h3 id="elapsed-communication-cost">Elapsed Communication Cost</h3>
<p>sum of the largest input  + output for any map process + output for any map process</p>
<h3 id="references">References</h3>
<ul>
<li><a href="http://web.stanford.edu/class/cs246/" target="_blank" rel="noopener">http://web.stanford.edu/class/cs246/</a></li>
<li><a href="http://www.mmds.org/#ver21" target="_blank" rel="noopener">http://www.mmds.org/#ver21</a></li>
</ul>
<h2 id="-----finding-simmilar-documnets-----">&mdash;- FINDING SIMMILAR DOCUMNETS &mdash;-</h2>
<h2 id="shingling-of-documents">Shingling of Documents</h2>
<p>The most effective way to represent documents as sets, for the purpose of identifying lexically similar documents is to construct from the document the set of short strings that appear within it.</p>
<h3 id="k-shingles-or-k-gram">k-Shingles or k-gram</h3>
<p>A document is a string of characters. Define a k-shingle for a document to be any substring of length k found within the document. Then, we may associate with each document the set of k-shingles that appear one or more times within that document. Instead of using substrings directly as shingles, we can pick a hash function that maps strings of length k to some number of buckets and treat the resulting bucket number as the shingle. The set representing a document is then the set of integers that are bucket numbers of one or more k-shingles that appear in the document. The result of hashing shingles also called <em>tokens</em>.</p>
<h2 id="similarity-preserving-summaries-of-sets">Similarity-Preserving Summaries of Sets</h2>
<h3 id="signatures">Signatures</h3>
<p>Sets of shingles are large. Even if we hash them to four bytes each, the space needed to store a set is still roughly four times the space taken by the document. If we have millions of documents, it may well not be possible to store all the shingle-sets in main memory.</p>
<p>Our goal in this section is to replace large sets by much smaller representations called “signatures.” The important property we need for signatures is
that we can compare the signatures of two sets and estimate the Jaccard similarity of the underlying sets from the signatures alone.</p>
<h3 id="minhashing">Minhashing</h3>
<p>The signatures we desire to construct for sets are composed of the results of a large number of calculations, say several hundred, each of which is a “minhash” of the characteristic matrix. In this section, we shall learn how a minhash is computed in principle, and in later sections we shall see how a good approximation to the minhash is computed in practice.</p>
<p>To minhash a set represented by a column of the characteristic matrix, pick a permutation of the rows. The minhash value of any column is the number of the first row, in the permuted order, in which the column has a 1. In the table bellow, you can see an example of minhash table with size 3. The input matrix is a matrix with binary input which has documents in the columns and shingles (tokens) in the rows and this input matrix I, has I<sub>ij</sub> =1, if document j contains token i. To calculate the minhash matrix, first we generate 3 different permutation of rows. Lets check the blue permutation. the first row in the blue permutation refers to the sixth row in the input matrix. In the sixth row we see that columns 1 and 3 has inputs 1, so we put the permutation row number (1 in this case) to the first and third columns of the signature vector. the second row in the blue permutation  refers to the fourth row in the input matrix. In the fourth row we see that columns 2 and 4 has inputs 1, so we put the permutation row number (2 in this case) to the second and fourth columns of the signature vector. As we filled all columns for the blue permutation, we can proceed to other permutations and fill the column values in the same manner.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/minhash.PNG" alt="Example of Minhashing" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h3 id="minhashing-and-jaccard-similarity">Minhashing and Jaccard Similarity</h3>
<p>There is a remarkable connection between minhashing and Jaccard similarity of the sets that are minhashed.</p>
<p>The probability that the minhash function for a random permutation of rows produces the same value for two sets equals the Jaccard similarity of those sets.</p>
<h3 id="computation-of-minhas-permutations">Computation of Minhas Permutations</h3>
<p>It is not feasible to permute a large characteristic matrix explicitly. Even picking a random permutation of millions or billions of rows is time-consuming, and the necessary sorting of the rows would take even more time.</p>
<p>Fortunately, it is possible to simulate the effect of a random permutation by a random hash function that maps row numbers to as many buckets as there are rows. A hash function that maps integers 0, 1, . . . , k −1 to bucket numbers 0 through k−1 typically will map some pairs of integers to the same bucket and leave other buckets unfilled. However, the difference is unimportant as long as k is large and there are not too many collisions. We can maintain the fiction that our hash function h “permutes” row r to position h(r) in the permuted order. Thus, instead of picking n random permutations of rows, we pick n randomly chosen hash functions h1, h2, . . . , hn on the rows. We construct the signature matrix by considering each row in their given order.</p>
<h2 id="locality-sensitive-hashing-lsh">Locality Sensitive Hashing (LSH)</h2>
<p>The general Idea of LSH is generating a small list of candidate pairs (pairs of elements whose similarity must be evaluated) from the collection of all elements. This is done via “hashing” items several times, in such a way that similar items are more likely to be hashed to the same bucket than dissimilar items are. We then consider any pair that hashed to the same bucket for any of the hashings to be a candidate pair. We check only the candidate pairs for similarity.</p>
<h3 id="lsh-for-minhash-signatures">LSH for Minhash Signatures</h3>
<p>Even though we can use minhashing to compress large documents into small signatures and preserve the expected similarity of any pair of documents, it still may be impossible to find the pairs with greatest similarity efficiently. The reason is that the number of pairs of documents may be too large, even if there are not too many documents. So we refer to the idea of LSH we mentioned above.</p>
<p>If we have minhash signatures for the items, an effective way to choose the hashings is to divide the signature matrix into b bands consisting of r rows each. For each band, there is a hash function that takes vectors of r integers (the portion of one column within that band) and hashes them to some large number of buckets. We can use the same hash function for all the bands, but we use a separate bucket array for each band, so columns with the same vector in different bands will not hash to the same bucket. In the figure bellow, you can see hash table for a single band in a signature matrix.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/lsh.PNG" alt="Example of LSH for minhash" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h3 id="analysis-of-the-banding-technique">Analysis of the Banding Technique</h3>
<p>Suppose we use b bands of r rows each, and suppose that a particular pair of documents have Jaccard similarity s. Recall that the probability the minhash signatures for these documents agree in any one particular row of the signature matrix is s. We can calculate the probability that these documents (or rather their signatures) become a candidate pair as follows:</p>
<ul>
<li>The probability that the signatures agree in all rows of one particular band is <em>s</em><sup><em>r</em></sup>.</li>
<li>The probability that the signatures disagree in at least one row of a particular band is 1- <em>s</em><sup><em>r</em></sup>.</li>
<li>The probability that the signatures disagree in at least one row of each of the bands is (1- <em>s</em><sup><em>r</em></sup>)<sup><em>b</em></sup> (also the probability of false negatives for s &gt; threshold).</li>
<li>The probability that the signatures agree in all the rows of at least one band, and therefore become a candidate pair, is 1- (1- <em>s</em><sup><em>r</em></sup>)<sup><em>b</em></sup>. (also the probability of false positives for s &lt; threshold).</li>
</ul>
<p>It may not be obvious, but regardless of the chosen constants b and r, this function has the form of an S-curve, as suggested in the figure bellow and an approximation for the threshold is (<em>1/b</em>)<sup><em>1/r</em></sup>. If you want a high recall value, it is better to have high <em>r</em> and low 0</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/lshrb.PNG" alt="LSH r and b" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h2 id="the-theory-of-locality-sensitive-functions">The Theory of Locality-Sensitive Functions</h2>
<p>Now, we shall explore other families of functions, besides the minhash functions, that can serve to produce candidate pairs efficiently. These functions can apply to the space of sets and the Jaccard distance, or to another space and/or another distance measure. There are three conditions that we need for a family of functions:</p>
<ol>
<li>They must be more likely to make close pairs be candidate pairs than distant pairs.</li>
<li>They must be statistically independent, in the sense that it is possible to estimate the probability that two or more functions will all give a certain response by the product rule for independent events.</li>
<li>They must be efficient, in two ways:
<ul>
<li>They must be able to identify candidate pairs in time much less than the time it takes to look at all pairs.</li>
<li>They must be combinable to build functions that are better at avoiding false positives and negatives, and the combined functions must also take time that is much less than the number of pairs.</li>
</ul>
</li>
</ol>
<h3 id="lsh-general-formulation">LSH General Formulation</h3>
<p>A collection of functions of this form will be called a family of functions. For example, the family of minhash functions, each based on one of the possible permutations of rows of a characteristic matrix, form a family.</p>
<p>Let d1 &lt; d2 be two distances according to some distance measure d. A family F of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in F:</p>
<ol>
<li>If d(x, y) ≤ d1, then the probability that f(x) = f(y) is at least p1.</li>
<li>If d(x, y) ≥ d2, then the probability that f(x) = f(y) is at most p2.</li>
</ol>
<h3 id="lsh-families-for-jaccard-distance">LSH Families for Jaccard Distance</h3>
<p>The family of minhash functions is a (d1, d2, 1−d1, 1−d2)-sensitive family for any d1 and d2, where 0 ≤ d1 &lt; d2 ≤ 1.</p>
<p>The reason is that if d(x, y) ≤ d1, where d is the Jaccard distance, then SIM(x, y) = 1 − d(x, y) ≥ 1 − d1. But we know that the Jaccard similarity of x and y is equal to the probability that a minhash function will hash x and y to the same value. A similar argument applies to d2 or any distance.</p>
<h3 id="lsh-families-for-hamming-distance">LSH Families for Hamming Distance</h3>
<p>Suppose we have a space of d-dimensional vectors, and h(x, y) denotes the Hamming distance between vectors x and y. If we take any one position of the vectors, say the ith position, we can define the function fi(x) to be the ith bit of vector x. Then f<sub><em>i</em></sub>(x) = f<sub><em>i</em></sub>(y) if and only if vectors x and
y agree in the ith position. Then the probability that f<sub><em>i</em></sub>(x) = f<sub><em>i</em></sub>(y) for a randomly chosen i is exactly 1 − h(x, y)/d; i.e., it is the fraction of positions in which x and y agree.</p>
<p>This situation is almost exactly like the one we encountered for minhashing. Thus, the family F consisting of the functions {f1, f2, . . . , fd} is a (d1, d2, 1 − d1/d, 1 − d2/d)-sensitive family of hash functions, for any d1 &lt; d2.</p>
<h3 id="lsh-families-for-cosine-distance">LSH Families for Cosine Distance</h3>
<p>Recall that the cosine distance between two vectors is the angle between the vectors. Note that these vectors may be in a space of many dimensions, but they always define a plane, and the angle between them is measured in this plane.</p>
<p>Given two vectors x and y, say f(x) = f(y) if and only if the dot products v<sub><em>f</em></sub> .x and v<sub><em>f</em></sub> .y have the same sign. Then F is a locality-sensitive family for the cosine distance. (d1, d2, (180 − d1)/180, (180 − d2)/180)-sensitive family of hash functions. From this basis, we can amplify the family as we wish, just as for the minhash-based family.</p>
<h4 id="sketches">Sketches</h4>
<p>Instead of choosing a random vector from all possible vectors, it turns out to be sufficiently random if we restrict our choice to vectors whose components are +1 and −1. The dot product of any vector x with a vector v of +1’s and −1’s is formed by adding the components of x where v is +1 and then subtracting the other components of x – those where v is −1. If we pick a collection of random vectors, say v1, v2, . . . , v<sub><em>n</em></sub>, then we can apply them to an arbitrary vector x by computing v1.x, v2.x, . . . , v<sub><em>n</em></sub>.x and then replacing any positive value by +1 and any negative value by −1. The result is called the sketch of x.</p>
<h2 id="------------mining-data-streams------------">&mdash;&mdash;&mdash;&ndash; MINING DATA STREAMS &mdash;&mdash;&mdash;&ndash;</h2>
<h2 id="filtering-data-streams">Filtering Data Streams</h2>
<h3 id="bloom-filters">Bloom Filters</h3>
<p>A Bloom filter consists of:</p>
<ol>
<li>An array of n bits, initially all 0’s.</li>
<li>A collection of hash functions h1, h2, . . . , hk. Each hash function maps “key” values to n buckets, corresponding to the n bits of the bit-array.</li>
<li>A set S of m key values.</li>
</ol>
<p>The purpose of the Bloom filter is to allow through all stream elements whose keys are in S, while rejecting most of the stream elements whose keys are not in S.</p>
<p>To initialize the bit array, begin with all bits 0. Take each key value in S and hash it using each of the k hash functions. Set to 1 each bit that is hi(K) for some hash function hi and some key value K in S. To test a key K that arrives in the stream, check that all of h1(K), h2(K), . . . , hk(K) are 1’s in the bit-array. If all are 1’s, then let the stream element through. If one or more of these bits are 0, then K could not be in S, so reject the stream element.</p>
<h3 id="analysis-of-bloom-filters">Analysis of Bloom Filters</h3>
<p>If a key value is in S, then the element will surely pass through the Bloom filter. However, if the key value is not in S, it might still pass. We need to understand how to calculate the probability of a false positive, as a function of n, the bit-array length, m the number of members of S, and k, the number of hash functions.</p>
<p>The model to use is throwing darts at targets. Suppose we have x targets and y darts. Any dart is equally likely to hit any target. After throwing the darts, how many targets can we expect to be hit at least once?</p>
<ul>
<li>
<p>The probability that a given dart will not hit a given target is (x − 1)/x.</p>
</li>
<li>
<p>The probability that none of the y darts will hit a target is ((x − 1)/x)<sup><em>y</em></sup>. We can write this expression as,
(1- 1/x)<sup><em>x(y/x)</em></sup>.</p>
</li>
<li>
<p>using the approximation (1-q)<sup><em>1/q</em></sup> = 1/e, for small q, we conclude that the probability that none of y darts hit a given target is e<sup><em>-y/x</em></sup>.</p>
</li>
<li>
<p>The optimal number of hash functions is (n/m) * ln(2).</p>
</li>
</ul>
<h2 id="counting-distinct-elements-in-a-stream">Counting Distinct elements in a Stream</h2>
<p>Data stream consists of a universe of elements chosen from a set of size N. The problem is maintaining a count of the number of distinct elements seen so far. Unfortunately, we do not have the space to keep the set of elements seen so far. So we need an unbiased estimator for this count. Here we introduce the Flajolet- Martin Algorithm.</p>
<h3 id="flajolet---martin-algorithm">Flajolet - Martin Algorithm</h3>
<ul>
<li>Pick a has function h, that maps each of N elements to at least log<sub><em>2</em></sub>N bits.</li>
<li>For each stream element a, let r(a) be the number of trailing zeros in h(a). And R = max<sub><em>a</em></sub>r(a), over all items a seen so far.</li>
<li>Estimated number of distinct element is = 2<sup><em>R</em></sup>.</li>
</ul>
<h3 id="analysis-of-flajolet---martin-algorithm">Analysis of Flajolet - Martin Algorithm</h3>
<p>This estimate makes intuitive sense. The probability that a given stream element a has h(a) ending in at least r 0’s is 2<sup><em>-r</em></sup>. Suppose there are m distinct elements in the stream. Then the probability that none of them has tail length at least r is (1-2<sup><em>-r</em></sup>)<sup><em>m</em></sup>. This probability is approximately, e<sup><em>-m2<sup>-r</sup></em></sup>.</p>
<ol>
<li>if m &laquo; 2<sup>r</sup>, then the probability that we shall find a tail of length at least r approaches 0.</li>
<li>if m &raquo; 2<sup>r</sup>, then the probability that we shall find a tail of length at least r approaches 1.</li>
<li>thus 2<sup>R</sup> will be always around m.</li>
</ol>
<p>The problem is that the probability halves when we increase R to R+1, but value doubles. We can use many hash functions h<sub>i</sub>. and get many samples of R<sub>i</sub> by first sorting all 2<sup>R</sup> values. After that;</p>
<ul>
<li>Partition your samples into small groups.</li>
<li>Take the median of groups.</li>
<li>Then take the average of the medians.</li>
</ul>
<h2 id="computing-moments">Computing Moments</h2>
<p>In this section we consider a generalization of the problem of counting distinct elements in a stream. The problem, called computing “moments,” involves the distribution of frequencies of different elements in the stream. We shall define moments of all orders and concentrate on computing second moments, from which the general algorithm for all moments is a simple extension.</p>
<h3 id="definition-of-moments">Definition of moments</h3>
<p>Suppose a stream consists of elements chosen from a universal set and assume the universal set is ordered. Let m<sub>i</sub> be the number of occurrences of the ith element for any i. Then kth moment of the stream is the sum over all i of (m<sub>i</sub>)<sup>k</sup>.</p>
<ul>
<li><strong><strong>0th moment</strong></strong> : number of distinct elements.</li>
<li><strong><strong>1st moment</strong></strong> : length of the stream.</li>
<li><strong><strong>2nd moment</strong></strong> : a measure of how uneven the distribution is.</li>
</ul>
<h3 id="ams-method">AMS Method</h3>
<p>Suppose we do not have enough space to count all the mi’s for all the elements of the stream. We can still estimate the second moment of the stream using a limited amount of space; the more space we use, the more accurate the estimate will be. We compute some number of <em>variables</em>. For each variable X, we store:</p>
<ol>
<li>A particular element of the universal set, which we refer to as <strong><strong>X.element</strong></strong>.</li>
<li>An integer <strong><strong>X.value</strong></strong>, which is the value of the variable. To determine the value of a variable X, we choose a position in the stream between 1 and n, uniformly and at random. Set <strong><strong>X.element</strong></strong> to be the element found there, and initialize  <strong><strong>X.value</strong></strong> to 1. As we read the stream, add 1 to  <strong><strong>X.value</strong></strong> each time we encounter another occurrence of <strong><strong>X.element</strong></strong> .</li>
</ol>
<h4 id="example">example:</h4>
<p>Suppose the stream is a, b, c, b, d, a, c, d, a, b, d, c, a, a, b. The length of the stream is n = 15. Since a appears 5 times, b appears 4 times, and c and d appear three times each, the second moment for the stream is 5<sup>2</sup>+4<sup>2</sup>+3<sup>2</sup>+3<sup>2</sup> = 59. Suppose we keep three variables, X1, X2, and X3. Also, assume that at “random” we pick the 3rd, 8th, and 13th positions to define these three variables.</p>
<p>When we reach position 3, we find element c, so we set <strong><strong>X1.element</strong></strong> = c and <strong><strong>X1.value</strong></strong>. Position 4 holds b, so we do not change X1. Likewise, nothing happens at positions 5 or 6. At position 7, we see c again, so we set <strong><strong>X1.value</strong></strong> = 2.</p>
<p>At position 8 we find d, and so set <strong><strong>X2.element</strong></strong> = d and <strong><strong>X2.value</strong></strong> = 1. Positions 9 and 10 hold a and b, so they do not affect X1 or X2. Position 11 holds d so we set <strong><strong>X2.value</strong></strong> = 2, and position 12 holds c so we set <strong><strong>X1.value</strong></strong> = 3.</p>
<p>At position 13, we find element a, and so set <strong><strong>X3.element</strong></strong> = a and <strong><strong>X3.value</strong></strong> = 1. Then, at position 14 we see another a and so set <strong><strong>X3.value</strong></strong> = 2. Position 15, with element b does not affect any of the variables, so we are done, with final values <strong><strong>X1.value</strong></strong> = 3 and <strong><strong>X2.value</strong></strong> = <strong><strong>X3.value</strong></strong> = 2.</p>
<p>We can derive an estimate of the second moment from any variable X. This estimate is <strong><strong>n(2X.value − 1)</strong></strong>.</p>
<h2 id="------dimensionality-reduction------">&mdash;&ndash; DIMENSIONALITY REDUCTION &mdash;&ndash;</h2>
<h2 id="eigenvalues-and-eigenvectors-of-symmetric-matrixes">Eigenvalues and Eigenvectors of Symmetric Matrixes</h2>
<p>Let <strong><strong>M</strong></strong> be a square matrix. Let <strong><strong>λ</strong></strong> be a constant and e a nonzero column vector with the same number of rows as <strong><strong>M</strong></strong>. Then <strong><strong>λ</strong></strong> is an eigenvalue of <strong><strong>M</strong></strong> and <strong><strong>e</strong></strong> is the corresponding eigenvector of <strong><strong>M</strong></strong> if <strong><strong>Me = λe</strong></strong>.</p>
<h3 id="some-properties-of-symmetric-matrices">Some properties of Symmetric Matrices</h3>
<ol>
<li>All eigenvalues of a symmetric matrix are real.</li>
<li>If x is a right eigenvector of M wit eigen value λ, x is also a left eigenvector for the same eigenvalue.</li>
<li>If M is symmetric, eigenvectors associated to different eigenvalues are mutually orthogonal.</li>
<li>Assume V is an orthonormal eigenvector basis for a symmetric matrix M. Then, V<sup>T</sup>V=VV<sup>T</sup>=I. This also implies V is invertible and its inver is V<sup>T</sup>.</li>
</ol>
<h3 id="using-eigenvectors-for-dimensionality-reduction">Using Eigenvectors for Dimensionality Reduction</h3>
<p>From the example we have just worked out, we can see a general principle. If M is a matrix whose rows each represent a point in a Euclidean space with any number of dimensions, we can compute M<sup>T</sup>M and compute its eigenpairs. Let E be the matrix whose columns are the eigenvectors, ordered as largest eigenvalue first. Define the matrix L to have the eigenvalues of M<sup>T</sup>M along the diagonal, largest first, and 0’s in all other entries. Then, since M<sup>T</sup>Me = λe = eλ for each eigenvector e and its corresponding eigenvalue λ, it follows that M<sup>T</sup>M = EL.</p>
<p>We observed that ME is the points of M transformed into a new coordinate space. In this space, the first axis (the one corresponding to the largest eigenvalue) is the most significant; formally, the variance of points along that axis is the greatest. The second axis, corresponding to the second eigenpair, is next most significant in the same sense, and the pattern continues for each of the eigenpairs. If we want to transform M to a space with fewer dimensions, then the choice that preserves the most significance is the one that uses the eigenvectors associated with the largest eigenvalues and ignores the other eigenvalues.</p>
<h2 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h2>
<p>We now take up a second form of matrix analysis that leads to a low-dimensional representation of a high-dimensional matrix. This approach, called singular- value decomposition (SVD), allows an exact representation of any matrix, and also makes it easy to eliminate the less important parts of that representation to produce an approximate representation with any desired number of dimensions.</p>
<h3 id="definition-of-svd">Definition of SVD</h3>
<p>Let M be an m × n matrix, and let the rank of M be r. Recall that the rank of a matrix is the largest number of rows (or equivalently columns) we can choose for which no nonzero linear combination of the rows is the all-zero vector 0 (we say a set of such rows or columns is independent). Then we can find matrices U, Sigma, and V with the following properties:</p>
<ol>
<li>U is an m × r column-orthonormal matrix ; that is, each of its columns is a unit vector and the dot product of any two columns is 0.</li>
<li>V is an n × r column-orthonormal matrix.</li>
<li>Sigma is a diagonal matrix; that is, all elements not on the main diagonal are 0. The elements of Sigma are called the singular values of M.</li>
</ol>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/SVD.PNG" alt="SVD" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
<h3 id="dimensionality-reduction-using-svd">Dimensionality Reduction Using SVD</h3>
<p>Suppose we want to represent a very large matrix M by its SVD components U, Sigma, and V, but these matrices are also too large to store conveniently. The best way to reduce the dimensionality of the three matrices is to set the smallest of the singular values to zero. If we set the s smallest singular values to 0, then we can also eliminate the corresponding s columns of U and V.</p>
<p>The choice of the lowest singular values to drop when we reduce the number of dimensions can be shown to minimize the root-mean-square error between the original matrix M and its approximation. Since the number of entries is fixed, and the square root is a monotone operation, we can simplify and compare the Frobenius norms of the matrices involved.</p>

    </div>

    








<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://copuronur.github.io/post/big_data_computing/&amp;text=Big%20Data%20computing" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://copuronur.github.io/post/big_data_computing/&amp;t=Big%20Data%20computing" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Big%20Data%20computing&amp;body=https://copuronur.github.io/post/big_data_computing/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://copuronur.github.io/post/big_data_computing/&amp;title=Big%20Data%20computing" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Big%20Data%20computing%20https://copuronur.github.io/post/big_data_computing/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://copuronur.github.io/post/big_data_computing/&amp;title=Big%20Data%20computing" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://copuronur.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/onur-copur/avatar_huef90ae86c0eb729c12c0a533f7cc8bb2_56889_270x270_fill_q75_lanczos_center.jpg" alt="Onur Copur"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://copuronur.github.io/">Onur Copur</a></h5>
      <h6 class="card-subtitle">MSc Data Science</h6>
      <p class="card-text">Data scientist &amp; Industrial Engineer</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/copuro_onur" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=o_SPbYQAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="fas fa-graduation-cap"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/CopurOnur" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/onurcopur/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  














  
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>

    
    
    
      
      
        <script src="https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js" integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    
      <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.7cd6ec29d281a73c92a2958a1584aadc.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
